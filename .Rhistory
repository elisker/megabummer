}
library(tidytext)
install.packages("tidytext")
load(url("http://varianceexplained.org/files/trump_tweets.rda"))
library(tidytext)
library(devtools)
devtools::install_github("juliasilge/tidytext")
library(tidytext)
nrc <- sentiments %>%
filter(lexicon  == "nrc") %>%
select(word,sentiment)
nrc
library(dplyr)
library(ggplot2)
library(ggthemes)
library(readr)
options(digits = 22) # to prevent tweet id from truncating
#full data set
tweets_df_all <- read_csv("Jan_2015-April_2016.csv")
setwd("~/Documents/Data Science/megabummer")
options(digits = 22) # to prevent tweet id from truncating
#full data set
tweets_df_all <- read_csv("Jan_2015-April_2016.csv")
names(tweets_df_all) <- c("id","username","text","date","geo","retweets","favorites","mentions","hashtags")
tweets_df_all <- tweets_df_all %>%
mutate(time=format(date, format="%H:%M:%S")) %>%
mutate(date2 = format(date, format="%m-%d-%y")) %>%
mutate(month = months(as.Date(date2,'%m-%d-%y'))) %>%
mutate(weekend = weekdays(as.Date(date2,'%m-%d-%y'))) %>%
mutate(weekend_binary = ifelse(weekend == "Saturday"|weekend=="Sunday"|weekend=="Friday", 1, 0)) %>%
filter(date2 != "12-31-14") %>%
filter(date < "2016-04-01")
#ggplot(tweets_df_all, aes(unclass(date))[1]) + geom_density()
# filter out duplicates
tweets_df_all <- tweets_df_all %>%
distinct(id)
#nrow(tweets_df_all)
tweets_df_all <- tweets_df_all %>%
distinct(text)
library(devtools)
#install_github("juliasilge/tidytext")
library(tidytext)
# other text mining: tm, quanteda
library(tidyr)
library(readr)
by_word <- tweets_df_all %>%
dplyr::select(text, id, date, date2, time, weekend, weekend_binary, month) %>%
unnest_tokens(word, text)
by_word_count <- by_word %>%
count(word, sort = TRUE)
head(by_word_count)
megabus_lexicon <- read_csv("megabus_lexicon.csv")
bing_megabus <- megabus_lexicon %>%
filter(lexicon %in% c("bing","megabummer")) %>%
dplyr::select(-score)
head(bing_megabus %>% filter(lexicon=="megabummer"))
mb_sentiment <- by_word %>%
inner_join(bing_megabus) %>%
mutate(score = ifelse(sentiment == "positive", 1, -1))
head(mb_sentiment %>% dplyr::select(id,word,sentiment,score))
library(data.table)
dt <- data.table(mb_sentiment)
mb_sentiment_tweet <- unique(dt[,list(score_tweet = sum(score), freq = .N, date, weekend_binary, date2, weekend, month), by = c("id")] )
tweets_df_all_joiner <- tweets_df_all %>% dplyr::select(id,text)
mb_sentiment_tweet <- left_join(mb_sentiment_tweet,data.table(tweets_df_all_joiner),by="id")
head(mb_sentiment_tweet)
mb_sentiment_date <- unique(mb_sentiment_tweet[,list(score_date = round(mean(score_tweet),2), freq = .N, weekend_binary, weekend, month), by = c("date2")] )
mb_sentiment_date <- mb_sentiment_date %>% filter(freq<500)
head(mb_sentiment_date)
positives <- bing_megabus %>%
filter(sentiment == "positive") %>%
dplyr::select(word)
negatives = bing_megabus %>%
filter(sentiment == "negative") %>%
dplyr::select(word)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
head(by_word)
word_list <- by_word %>% dplyr::select(word)
head(word_list)
word_list_negatives <- subset(word_list, word %in% negatives$word)
head(word_list_negatives)
word_list_positives <- subset(word_list, word %in% positives$word)
head(word_list_positives)
word_list_negatives <- Corpus(VectorSource(word_list_negatives))
inspect(word_list_negatives)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
word_list_negatives <- tm_map(word_list_negatives, toSpace, "/")
word_list_negatives <- tm_map(word_list_negatives, toSpace, "@")
word_list_negatives <- tm_map(word_list_negatives, toSpace, "\\|")
dtm <- TermDocumentMatrix(word_list_negatives)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
word_list_positives <- Corpus(VectorSource(word_list_positives))
#inspect(word_list_positives)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
word_list_positives <- tm_map(word_list_positives, toSpace, "/")
word_list_positives <- tm_map(word_list_positives, toSpace, "@")
word_list_positives <- tm_map(word_list_positives, toSpace, "\\|")
word_list_positives <- tm_map(word_list_positives, removeWords, c("megabus", "the", "and", "https", "you", "t.co", "for", "this", "bus", "that"))
#Build a term-document matrix
dtm <- TermDocumentMatrix(word_list_positives)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
# Word Cloud (Positive)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
# Word Clouds by Weekend vs. Non-Weekend
# Create Word List for Weekends
head(by_word)
word_list <- by_word %>% dplyr::select(word, weekend_binary)
head(word_list)
word_list <- subset(word_list, word %in% bing_megabus$word)
word_list_weekend <- word_list %>%
filter(weekend_binary==1)
# Create Word List for Weekdays
word_list_nonweekend <- word_list %>%
filter(weekend_binary==0)
head(word_list_nonweekend)
# Word Cloud for Weekend
word_list_weekend <- Corpus(VectorSource(word_list_weekend))
#inspect(word_list_weekend)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
word_list_weekend <- tm_map(word_list_weekend, toSpace, "/")
word_list_weekend <- tm_map(word_list_weekend, toSpace, "@")
word_list_weekend <- tm_map(word_list_weekend, toSpace, "\\|")
word_list_weekend <- tm_map(word_list_weekend, removeWords, c("megabus", "the", "and", "https", "you", "t.co", "for", "this", "bus", "that"))
#Build a term-document matrix
dtm <- TermDocumentMatrix(word_list_weekend)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
# Word Cloud (Weekend)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
word_list_nonweekend <- Corpus(VectorSource(word_list_nonweekend))
inspect(word_list_nonweekend)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
word_list_nonweekend <- tm_map(word_list_nonweekend, toSpace, "/")
word_list_nonweekend <- tm_map(word_list_nonweekend, toSpace, "@")
word_list_nonweekend <- tm_map(word_list_nonweekend, toSpace, "\\|")
word_list_nonweekend <- tm_map(word_list_nonweekend, removeWords, c("megabus", "the", "and", "https", "you", "t.co", "for", "this", "bus", "that"))
#Build a term-document matrix
dtm <- TermDocumentMatrix(word_list_nonweekend)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
# Word Cloud (Weekend)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
word_cooccurences <- by_word %>% dplyr::select(word, id)
word_cooccurences <- subset(word_cooccurences, word %in% bing_megabus$word)
word_cooccurences <- word_cooccurences %>%
pair_count(id, word, sort = TRUE) %>%
dplyr::filter(n>25)
word_cooccurences
#}
library(devtools)
library(igraph)
library(ggraph)
set.seed(2016)
word_cooccurences %>%
filter(n >= 10) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour="gray") +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1.8, size=5) +
theme_void()
by_word <- tweets_df_all %>%
dplyr::select(text, id, date, date2, time, weekend, weekend_binary, month) %>%
unnest_tokens(word, text)
word_list <- by_word %>% dplyr::select(word, date2)
word_list <- subset(word_list, word %in% bing_megabus$word)
word_list_date <- word_list %>%
filter(date2=="05-13-15")
word_list_date <- word_list_date %>% dplyr::select(word)
word_list_date <- Corpus(VectorSource(word_list_date))
#inspect(word_list_date)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
word_list_date <- tm_map(word_list_date, toSpace, "/")
word_list_date <- tm_map(word_list_date, toSpace, "@")
word_list_date <- tm_map(word_list_date, toSpace, "\\|")
word_list_date <- tm_map(word_list_date, removeWords, c("megabus", "the", "and", "https", "you", "t.co", "for", "this", "bus", "that"))
#Build a term-document matrix
dtm <- TermDocumentMatrix(word_list_date)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
d
# Word Cloud (Date)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
by_word <- tweets_df_all %>%
dplyr::select(text, id, date, date2, time, weekend, weekend_binary, month) %>%
unnest_tokens(word, text)
word_list <- by_word %>% dplyr::select(word, date2)
word_list <- subset(word_list, word %in% bing_megabus$word)
word_list_date <- word_list %>%
filter(date2=="04-13-15")
word_list_date <- word_list_date %>% dplyr::select(word)
word_list_date <- Corpus(VectorSource(word_list_date))
#inspect(word_list_date)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
word_list_date <- tm_map(word_list_date, toSpace, "/")
word_list_date <- tm_map(word_list_date, toSpace, "@")
word_list_date <- tm_map(word_list_date, toSpace, "\\|")
word_list_date <- tm_map(word_list_date, removeWords, c("megabus", "the", "and", "https", "you", "t.co", "for", "this", "bus", "that"))
#Build a term-document matrix
dtm <- TermDocumentMatrix(word_list_date)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
# Word Cloud (Date)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
by_word <- tweets_df_all %>%
dplyr::select(text, id, date, date2, time, weekend, weekend_binary, month) %>%
unnest_tokens(word, text)
word_list <- by_word %>% dplyr::select(word, date2)
word_list <- subset(word_list, word %in% bing_megabus$word)
word_list_date <- word_list %>%
filter(date2=="02-21-16")
word_list_date <- word_list_date %>% dplyr::select(word)
word_list_date <- Corpus(VectorSource(word_list_date))
#inspect(word_list_date)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
word_list_date <- tm_map(word_list_date, toSpace, "/")
word_list_date <- tm_map(word_list_date, toSpace, "@")
word_list_date <- tm_map(word_list_date, toSpace, "\\|")
word_list_date <- tm_map(word_list_date, removeWords, c("megabus", "the", "and", "https", "you", "t.co", "for", "this", "bus", "that"))
#Build a term-document matrix
dtm <- TermDocumentMatrix(word_list_date)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
# Word Cloud (Date)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
by_word <- tweets_df_all %>%
dplyr::select(text, id, date, date2, time, weekend, weekend_binary, month) %>%
unnest_tokens(word, text)
word_list <- by_word %>% dplyr::select(word, date2)
word_list <- subset(word_list, word %in% bing_megabus$word)
word_list_date <- word_list %>%
filter(date2=="02-14-16"|date2=="02-14-15")
word_list_date <- word_list_date %>% dplyr::select(word)
word_list_date <- Corpus(VectorSource(word_list_date))
#inspect(word_list_date)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
word_list_date <- tm_map(word_list_date, toSpace, "/")
word_list_date <- tm_map(word_list_date, toSpace, "@")
word_list_date <- tm_map(word_list_date, toSpace, "\\|")
word_list_date <- tm_map(word_list_date, removeWords, c("megabus", "the", "and", "https", "you", "t.co", "for", "this", "bus", "that"))
#Build a term-document matrix
dtm <- TermDocumentMatrix(word_list_date)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
# Word Cloud (Date)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
warnings()
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, scale=c(4,0.2), rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, scale=c(2,0.2), rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, scale=c(2),0.5), rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, scale=c(2,0.5), rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, scale=c(2,2), rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, scale=c(3,.2), rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
word_cooccurences <- by_word %>% dplyr::select(word, id) %>% filter()
word_cooccurences <- subset(word_cooccurences, word %in% bing_megabus$word)
word_cooccurences <- word_cooccurences %>%
pair_count(id, word, sort = TRUE) %>%
dplyr::filter(n>1) %>%
filter(date2=="02-14-16"|date2=="02-14-15")
set.seed(2016)
word_cooccurences %>%
filter(n >= 10) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour="gray") +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1.8, size=5) +
theme_void()
word_cooccurences <- by_word %>%
filter(date2=="02-14-16"|date2=="02-14-15") %>%
dplyr::select(word, id) %>% filter()
word_cooccurences <- subset(word_cooccurences, word %in% bing_megabus$word)
word_cooccurences <- word_cooccurences %>%
pair_count(id, word, sort = TRUE) %>%
dplyr::filter(n>1)
set.seed(2016)
word_cooccurences %>%
filter(n >= 10) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour="gray") +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1.8, size=5) +
theme_void()
word_cooccurences <- by_word %>%
#filter(date2=="02-14-16"|date2=="02-14-15") %>%
dplyr::select(word, id) %>% filter()
word_cooccurences <- subset(word_cooccurences, word %in% bing_megabus$word)
word_cooccurences <- word_cooccurences %>%
pair_count(id, word, sort = TRUE) %>%
dplyr::filter(n>1)
set.seed(2016)
word_cooccurences %>%
filter(n >= 10) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour="gray") +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1.8, size=5) +
theme_void()
word_cooccurences <- by_word %>% dplyr::select(word, id)
word_cooccurences <- subset(word_cooccurences, word %in% bing_megabus$word)
word_cooccurences <- word_cooccurences %>%
pair_count(id, word, sort = TRUE) %>%
dplyr::filter(n>1) %>%
filter(date2=="02-14-16"|date2=="02-14-15")
set.seed(2016)
word_cooccurences %>%
filter(n >= 10) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour="gray") +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1.8, size=5) +
theme_void()
word_cooccurences <- by_word %>% dplyr::select(word, id, date2)
word_cooccurences <- subset(word_cooccurences, word %in% bing_megabus$word)
word_cooccurences <- word_cooccurences %>%
pair_count(id, word, sort = TRUE) %>%
dplyr::filter(n>1) %>%
filter(date2=="02-14-16"|date2=="02-14-15")
set.seed(2016)
word_cooccurences %>%
filter(n >= 10) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour="gray") +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1.8, size=5) +
theme_void()
word_cooccurences <- by_word %>% dplyr::select(word, id, date2)
word_cooccurences <- subset(word_cooccurences, word %in% bing_megabus$word)
word_cooccurences <- word_cooccurences %>%
pair_count(id, word, sort = TRUE) %>%
dplyr::filter(n>20)
set.seed(2016)
word_cooccurences %>%
filter(n >= 10) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour="gray") +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1.8, size=5) +
theme_void()
word_cooccurences <- by_word %>% dplyr::select(word, id, date2)
word_cooccurences <- subset(word_cooccurences, word %in% bing_megabus$word)
word_cooccurences <- word_cooccurences %>%
pair_count(id, word, sort = TRUE) %>%
dplyr::filter(n>20)
set.seed(2016)
word_cooccurences %>%
filter(n >= 20) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour="gray") +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1.8, size=5) +
theme_void()
word_cooccurences <- by_word %>% dplyr::select(word, id, date2)
word_cooccurences <- subset(word_cooccurences, word %in% bing_megabus$word)
word_cooccurences <- word_cooccurences %>%
pair_count(id, word, sort = TRUE) %>%
dplyr::filter(n>25)
set.seed(2016)
word_cooccurences %>%
filter(n >= 20) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour="gray") +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1.8, size=5) +
theme_void()
word_cooccurences <- by_word %>% dplyr::select(word, id, date2)
word cooccurences <- word_cooccurences %>%
filter(date2=="02-14-16"|date2=="02-14-15")
word_cooccurences <- subset(word_cooccurences, word %in% bing_megabus$word)
word_cooccurences <- word_cooccurences %>%
pair_count(id, word, sort = TRUE) %>%
dplyr::filter(n>1)
set.seed(2016)
word_cooccurences %>%
filter(n >= 10) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour="gray") +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1.8, size=5) +
theme_void()
word_cooccurences <- by_word %>% dplyr::select(word, id, date2)
word_cooccurences <- word_cooccurences %>%
filter(date2=="02-14-16"|date2=="02-14-15")
word_cooccurences <- subset(word_cooccurences, word %in% bing_megabus$word)
word_cooccurences <- word_cooccurences %>%
pair_count(id, word, sort = TRUE) %>%
dplyr::filter(n>1)
set.seed(2016)
word_cooccurences %>%
filter(n >= 10) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour="gray") +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1.8, size=5) +
theme_void()
options(digits = 3)
ggplot(data=mb_sentiment_date, aes(x=freq, y=score_date)) +
geom_line() + xlab("Number of megabus tweets") +
ylab("Tweet sentiment score") +
ggtitle("Tweet sentiment as a function of tweet volume") +
theme_hc()
options(digits = 22)
ggplot(mb_sentiment_tweet, aes(x=weekend_binary, y=score_tweet, group=weekend_binary)) +
geom_boxplot(aes(fill=weekend_binary)) +
xlab("Non-weekend vs weekend") +
ylab("Tweet sentiment score") +
ggtitle("Variation in tweet sentiment, weekends v non-weekends") +
geom_jitter(colour="gray40",
position=position_jitter(width=0.2), alpha=0.3)
options(digits = 3)
ggplot(mb_sentiment_tweet, aes(x=weekend_binary, y=score_tweet, group=weekend_binary)) +
geom_boxplot(aes(fill=weekend_binary)) +
xlab("Non-weekend vs weekend") +
ylab("Tweet sentiment score") +
ggtitle("Variation in tweet sentiment, weekends v non-weekends") +
geom_jitter(colour="gray40",
position=position_jitter(width=0.2), alpha=0.3)
options(digits = 22)
options(digits = 3)
ggplot(mb_sentiment_holidays, aes(x=holiday, y=score_date, group=holiday)) +
geom_boxplot(aes(fill=holiday)) +
xlab("Non-holiday vs. holiday") +
ylab("Tweet sentiment score") +
ggtitle("Variation in tweet sentiment score between holidays and non-holidays") +
geom_jitter(colour="gray40",
position=position_jitter(width=0.2), alpha=0.3)
options(digits = 22)
?wordcloud()
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=150, random.order=FALSE, scale=c(1,1), rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=150, random.order=FALSE, scale=c(1,0.5), rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=150, random.order=FALSE, scale=c(2,0.5), rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=150, random.order=FALSE, scale=c(2,2), rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=150, random.order=FALSE, scale=c(3,0.5), rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=125, random.order=FALSE, scale=c(3,0.5), rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=125, random.order=FALSE, scale=c(4,0.5), rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
