---
title: "Megabummer"
output: html_document
---

### Data Science BIO 260 Final Project, Spring 2016
##### Allison Blajda, Leo Brown, and Emily Lisker

### Overview and Motivation

Social media users leave digital traces that can be leveraged to guide consumer purchasing decisions. Twitter users, in particular, provide temporal data that can be analyzed to develop a more precise understanding of usersâ€™ perspectives or sentiments about a service, company, or product. This information, in turn, can be shared with the public to allow customers to make decisions based on crowd-sourced knowledge. It may also even help the service provider understand how they can improve their services.

Imagine it is Friday in July and you are finishing up a long week at the office. The clock reads 4:00pm and you quickly race to pack up your work station. You need to get downtown to catch the 5pm MegaBus from NYC to Washington, DC. You are ready to start the weekend and have plans at 10pm in downtown DC. You navigate the NYC subway and break a sweat walking quickly down 34th Street, hoping that you are early enough to score an aisle seat. As you approach the bus stop you come into view of your worst nightmare: a line with hundreds of angry Megabus passengers waiting with no bus in sight. As you walk towards the end of the line another passenger mentions that the 3:30pm bus hasn't arrived yet. A Megabus employee tells you he has no idea when the 5pm bus will depart ("Who knows..."). It is 90 degrees, you are sweating profusely, and you will surely miss your evening plans. You feel helpless. A wronged consumer with no recourse. Then you remember a social media site called Twitter...

Based on our personal experience and the experience of others, we aim to analyze recent tweets to understand the degree of positive and negative sentiment among users of the low-cost bus company, Megabus, over time. While Megabus is an inexpensive and convenient transportation option with many buses departing major cities each day, we hypothesized that some days or times of year may be more reliable, i.e., buses have fewer breakdowns and delays, than others. Using social media, data visualization and statistics we provide an informative and entertaining sentiment analysis of Megabus experiences from January 1, 2016 to April 1, 2016. Our work offers future bus riders information on the type of experience they can expect when riding the Megabus.


### Related Work: 

Sentiment analysis performed by our very own TA, David Robinson provided initial guiding inspiration for this this work (https://github.com/juliasilge/tidytext).

We were also inspired by sentiment analysis of Twitter data related to current events. Some examples of other analyses that we explored includes: http://www.wired.com/2015/02/best-worst-public-transit-systems-according-twitter/#slide-2 and https://medium.com/mit-media-lab/introducing-tonar-3bf161cba369#.gixrkibqj

### Initial Questions: 

As a team, we were originally interested in building a predictive model for Megabus experience based on time of day, location and bus route. Based on the data that we were able to collect from Twitter, these questions evolved to focus on the relationship between Megabus passenger sentiment, the volume of Megabus tweets, day of the week, whether the tweet was posted on a weekday (Monday-Thursday) or a weekend (Friday-Sunday), and month of the year.  

The questions we have answered as part of this analysis include the following:
1. How can we most effectively scrape Twitter for data related to Megabus sentiment?
2. What is overall Megabus sentiment and volume of Megabus tweets during the period from January 1, 2015 to April 1, 2016?
3. How does Megabus sentiment and volume of tweets differ based on day of the week and month of the year?
4. What is the realtionship between volume of Megabus tweets on a given day and the overall sentiment we expect to see?

### Data: 

#### Data Scraping and Tidying
We aimed to obtain historical tweet data in order to observe trends in tweet volume and sentiment over time and identify associations with factors such as time of year, month, and day. Since there are many factors influencing tweet volume and sentiment, we wished to obtain the largest possible data set.

Initially, we set up an app (actually, three apps, one for each of us) on the Twitter API that obtained tweets including the word "megabus." This was a great learning experience; we remember fondly this effort as our first true collaboration on git; our first exposure to the Twitter API; when we first set up an .Rprofile to store key sensitive variables; and the first tweet data we obtained. However, it became clear that the Twitter API would only provide us with tweets from the last week. This was insufficient for most of the trends we wished to consider.

A google search led us to [Jefferson Henrique's python package GetOldTweets-python](https://github.com/Jefferson-Henrique/GetOldTweets-python). This package enabled us to obtain tweets from any date (at least, we did not encounter a date that was inaccessible). The original package can be run to print tweets in the terminal. We modified the script so that it would print tweets to a csv file. As a result we obtained over 100,000 tweets from January 1, 2015 through April 1, 2016 containing the word "megabus."

We conducted much of our exploratory analysis using only tweets from the first three months of 2015. Though some questions of interest pertained to the entire 15-month period, we wanted to make sure the data frame was not too large during our exploratory phase. There was initially some evidence that the data frame was too large because some commands were slow. However, it was later identified that the only time-consuming command was an inefficiently written for loop that created several new columns. This was improved to a series of mutate() functions, and since then, wrangling the data frames takes only a few seconds from start to finish.

In order to look at variations in sentiment over days and time, we parsed the combined date and time variable into two new variables, containing just date and time information. We also used the months() function to create a month column and the weekdays() function to create a column that identified the tweet day and another to identify the tweet's status binary weekend status, with Friday counting as part of the weekend.

We cleaned the data using dplyr and the tidytext package, in order to help with the text mining tasks necessary for sentiment analysis, available here: https://github.com/juliasilge/tidytext. 



####Loading all necessary libraries
```{r}
library(dplyr)
library(ggplot2)
library(ggthemes)
library(readr)
library(devtools)
library(tidytext)
library(tidyr)
library(readr)
library(data.table)
library(Hmisc)

library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)

library(devtools)

library(igraph)
library(ggraph)
library(gapminder)

library(car)
library(MASS)
```
####Loading and wrangling the full data set
```{r}
#Increase digits so that the many-digit tweet id does not cause a problem.
options(digits = 22) 
tweets_df_all <- read_csv("Jan_2015-April_2016.csv")
#Change the column names.
names(tweets_df_all) <- c("id","username","text","date","geo","retweets","favorites","mentions","hashtags")

#get a subset of random lines from the full set to work with:
#tweets_df_all <- tweets_df_all[sample(1:nrow(tweets_df_all), 10000, replace=FALSE),]

tweets_df_all <- tweets_df_all %>%
  mutate(time=format(date, format="%H:%M:%S")) %>%
  mutate(date2 = format(date, format="%m-%d-%y")) %>%
  mutate(month = months(as.Date(date2,'%m-%d-%y'))) %>%
  mutate(weekend = weekdays(as.Date(date2,'%m-%d-%y'))) %>%
  mutate(weekend_binary = ifelse(weekend == "Saturday"|weekend=="Sunday"|weekend=="Friday", 1, 0)) %>% 
  filter(date2 != "12-31-14") %>%
  filter(date < "2016-04-01")
# filter out duplicates
tweets_df_all <- tweets_df_all %>%
  distinct(id)
#nrow(tweets_df_all)
tweets_df_all <- tweets_df_all %>%
  distinct(text)
```
####Exploring the data set, part 1 (prior to sentiment analysis)
```{r}
# explore number of tweets per user including megabus handles
prolific_tweeters_all <- tweets_df_all %>% 
  group_by(username) %>%
  summarise(tweets = n()) %>%
  arrange(desc(tweets)) 

# filter out tweets from megabus operators
tweets_df_all = tweets_df_all[!grepl("megabus|megabusuk|MegabusHelp|megabusit|megabusde|megabusGold", tweets_df_all$username),]
prolific_tweeters_filtered <- tweets_df_all %>% 
  group_by(username) %>%
  summarise(tweets = n()) %>%
  arrange(desc(tweets))
```

### Exploratory Analysis

#####Histogram of number of tweets
```{r}
ggplot(filter(prolific_tweeters_filtered, tweets>0), aes(tweets)) + 
  geom_histogram(binwidth = 1) + xlab("Number of megabus tweets per user") + ylab("Number of users") + theme_hc()
```

#####Histogram of number of tweets, for users with more than 8 tweets so that we can get a closer look at the more prolific tweeters
```{r}
ggplot(filter(prolific_tweeters_filtered, tweets>8), aes(tweets)) + 
  geom_histogram(binwidth = 1) + xlab("Number of megabus tweets per user") + ylab("Number of users") + theme_hc()
```

#####Tweets per day
```{r}
ggplot(data=tweets_df_all, aes(x=as.Date(date2,'%m-%d-%y'))) + 
  geom_histogram(aes(fill=..count..), binwidth=1) + 
  scale_x_date("Date") + 
  scale_y_continuous("Frequency")

#The outlying days with high tweet volume are:
tweets_df_all %>% group_by(date2) %>% count(date2, sort = TRUE) %>% filter(n>500)
#The outlying days with low tweet volume are:
tweets_df_all %>% group_by(date2) %>% count(date2, sort = TRUE) %>% filter(n<100)
```

On each of the outlying days for high tweet volume, there were significant news stories about Megabus. More on that below.

#### Sentiment Analysis
Like most sentiment analyses, we relied on a lexicon of positive and negative words. We started with the Bing lexicon in the tidytext package in the sentiment dataset (described here in more detail https://www.cs.uic.edu/~liub/). This lexicon has been compiled over the past 12 years by researchers at the University of Illinois at Chicago. 

After initial review of the Bing lexicon, we realized that we need to supplement the list of positive and negative words in order to capture the extent of Megabus transportation-specific sentiments. We added a total of 57 sentiment related to Megabus and transportation experience. These additional negative and positive words were identified  by manually reviewing a random sample from 2900 tweets containing the keyword, "megabus" queried on 4/20/2016, following methods similar to those described here: http://www.wired.com/2015/02/best-worst-public-transit-systems-according-twitter/. This review comparing the Bing lexicon with Megabus tweets also revealed that a few sentiments already in the lexicon needed to be recorded or removed. These included: changing "uneventful" from negative to positive, reversing "cheap" from negative to positive given Megabus's platform as a cheap transportation provider, and the removal of "like" which could be positive or negative and is probaby slang.

```{r}
by_word <- tweets_df_all %>%
  dplyr::select(text, id, date, date2, time, weekend, weekend_binary, month) %>%
  unnest_tokens(word, text) 

# look at most commonly tweeted words
by_word_count <- by_word %>%
  count(word, sort = TRUE) 
head(by_word_count)

megabus_lexicon <- read_csv("megabus_lexicon.csv")


# create new dataframe of bing and megabummer sentiments
bing_megabus <- megabus_lexicon %>%
  filter(lexicon %in% c("bing","megabummer")) %>%
  dplyr::select(-score)
head(bing_megabus %>% filter(lexicon=="megabummer"))

# join tweets with sentiment and add score column
mb_sentiment <- by_word %>%
  inner_join(bing_megabus) %>%
  mutate(score = ifelse(sentiment == "positive", 1, -1))
head(mb_sentiment %>% dplyr::select(id,word,sentiment,score))

# calculate score for each tweet
dt <- data.table(mb_sentiment)

#build data set for sentiment analysis containing data on each tweet including sentiment score
mb_sentiment_tweet <- unique(dt[,list(score_tweet = sum(score), freq = .N, date, weekend_binary, date2, weekend, month), by = c("id")] )
tweets_df_all_joiner <- tweets_df_all %>% dplyr::select(id,text)
mb_sentiment_tweet <- left_join(mb_sentiment_tweet,data.table(tweets_df_all_joiner),by="id")
head(mb_sentiment_tweet)

#Creating data table of calendar dates, including weekend status, day of week (column name weekend), month, and tweet frequency and sentiment
mb_sentiment_date <- unique(mb_sentiment_tweet[,list(score_date = round(mean(score_tweet),2), freq = .N, weekend_binary, weekend, month), by = c("date2")] )
mb_sentiment_date <- mb_sentiment_date %>% filter(freq<500)
head(mb_sentiment_date)

#Creating data table of calendar dates and tweet frequency and sentiment with holiday status (including federal holidays, Valentine's Day, and Halloween)
mb_sentiment_holidays <- mb_sentiment_date %>% 
  mutate(holiday = ifelse(date2 == "01-01-15" |
                            date2 == "01-19-15" |
                            date2 == "02-14-15" |
                            date2 == "02-16-15" |
                            date2 == "05-25-15" |
                            date2 == "09-07-15" |
                            date2 == "10-12-15" |
                            date2 == "10-31-15" |
                            date2 == "11-11-15" |
                            date2 == "11-26-15" |
                            date2 == "12-25-15" |
                            date2 == "01-01-16" |
                            date2 == "01-18-16" |
                            date2 == "02-14-16" |
                            date2 == "02-15-16",1,
                          0
                            ))
head(mb_sentiment_holidays)
```

####Exploring the data set, part 2 (sentiment analysis)
The describe() function in Hmisc helped us see how tweet sentiment scores are distributed. In particular, we noticed that the distribution of *by tweet* sentiment scores was similar to the distribution of *by date* sentiment scores (the average sentiment score for that day). We therefore focused our analysis on the *by date* sentiment scores because it was a convenient bin for looking at trends over time and for moderating the considerable variance. The *by date* sentiment score gives a sense for the overall level of sentiment on any given day, which is what we ultimately decided to consider as our main outcome.
```{r}
describe(mb_sentiment_tweet)
describe(mb_sentiment_date)
describe(mb_sentiment_holidays)
```

Looking at a line graph of the sentiment score over time is not particularly useful--it is just a blur with much more variability than any clear trend.

```{r}
ggplot(data=mb_sentiment_tweet, aes(x=date, y=score_tweet)) + 
  geom_line()
```

However, with smoothing, ebbs and flows over time become clear. Though there is much variance day to day, smoothing reveals broad seasonal trends. Smoothing is appropriate for tweet volume and sentiment because the outcome is continuous.

```{r}
options(digits = 3) 
ggplot(data=mb_sentiment_tweet, aes(x=date, y=score_tweet)) + 
  geom_smooth()
options(digits = 22) 
```

This chart shows the distribution of daily sentiment scores. The tweets of most days aired on the negative side, with few days boasting an average tweet score above 0.

```{r}
options(digits = 3)
ggplot(data=mb_sentiment_date, aes(score_date)) + 
  geom_histogram(binwidth = 0.1)
options(digits = 22)
```

####Hypothesis testing
We first tested the relationship between tweet volume and tweet sentiment. We expected there to be a correlation, and there was. A linear regression was appropriate for this because while we are uncertain about causality (and indeed, causality may not even be consistent), we had reason to believe that there was a relationship between tweet volume and sentiment, both based on our data exploration.

```{r}
#Hyp. #1: tweet sentiment on low volume days = on high volume days
h1.lm <- lm(score_date ~ freq, data = mb_sentiment_date)
summary(h1.lm)
#REJECT THE NULL (STRONGLY), conclude that tweet sentiment on low volume days > tweet sentiment on high volume days
# The volume of tweets on a given day is a statistically significant predictor of the average daily sentiment score and that for every additional tweet, we would expect a 0.001 decrease in average daily sentiment score.

ggplot(data=mb_sentiment_date, aes(x=freq, y=score_date)) + 
  geom_line() + xlab("Number of megabus tweets") + 
  ylab("Tweet sentiment score") + 
  ggtitle("Tweet sentiment as a function of tweet volume") + 
  theme_hc()
#Commentary:
#While we have found a highly statistically significant trend, the effect is moderate considering the degree of variance in tweet sentiment. In order to explore the meaning and practical application of the relationship, we would need to further examine days with different sentiments of tweets and consider whether the difference as measured by our scoring system is meaningful.

score_date.res = resid(h1.lm)
plot(mb_sentiment_date$freq, score_date.res, 
ylab="Residuals", xlab="Number of Tweets", 
main="Tweets and Megabus Sentiment")
abline(0,0)
#There is homoscadesticity of residuals, therefore linear regression is an appropriate method of analysis.

ncvTest(h1.lm)
# We fail to reject the null hypothesis of homoskedastic errors

# Normality of Residuals
#As demonstrated in the QQ-Plot below of the studendized residuals, the linearity of the points suggests that the residuals are normally distributed and further confirms that linear regression is appropriate for our dataset.
qqPlot(h1.lm, main="QQ Plot")

# distribution of studentized residuals
# We can also use a combination of a density plot and histogram to visualize that the normality assumption holds true as demonstrated below.

sresid <- studres(h1.lm) 
hist(sresid, freq=FALSE, 
     main="Distribution of Studentized Residuals")
xfit<-seq(min(sresid),max(sresid),length=40) 
yfit<-dnorm(xfit) 
lines(xfit, yfit)
```

We then tested hypotheses about the relationship between average tweet sentiment and unit of time (day of the week, month of the year). We included frequency (volume) as a covariate since it has been established that there is a relationship between tweet volume and sentiment.

```{r}
#Hyp. 2: Sentiment ~ day of the week when stratifying on freq.
fit <- lm(score_date ~ weekend + freq, data=mb_sentiment_date)
summary(fit)

#REJECT THE NULL, conclusion: The volume of tweets on a given day is a statistically significant predictor of the average daily sentiment score. For every additional tweet, we would expect a 0.001 decrease in average daily sentiment score. The days of the week are not significantly associated with the average daily sentiment score. 

#Hyp. #2a: sentiment ~ month when stratifying on freq.
fit <- lm(score_date ~ month + freq, data=mb_sentiment_date)
summary(fit) # show results

#REJECT THE NULL, conclusion: When we stratify on volume of tweets, we see that month is a statistically  significant predictor of the average daily sentiment score in some cases. The months of December, June, and October are statistically significantly associated with average daily sentiment score. Looking at the month of December, for example, December we would expect, on average, a 0.127 decrease in average daily sentiment for every additional tweet.
```

We then explored the relationship between weekend (Friday, Saturday, or Sunday) and tweet sentiment. First, we looked at the average tweet score for all tweets occurring on weekends vs. non-weekends. Then, we looked at the average of the daily tweet scores, each of which is the average sentiment for all tweets on that day. The first approach treats all tweets equally, while the second approach avoids overrepresenting days with many tweets, such as days when a significant event occurs.

```{r}
#Hyp. #3a: sentiment weekend = sentiment weekday (each weekend day is weighted by tweet volume)
the_weekend = mb_sentiment_tweet %>% filter(weekend_binary == 1)
not_the_weekend = mb_sentiment_tweet %>% filter(weekend_binary == 0)
var.test(the_weekend$score_tweet,not_the_weekend$score_tweet)#variances are equal if p-value > 0.05
t.test(the_weekend$score_tweet,not_the_weekend$score_tweet)#,var.equal = TRUE)
#Conclusion: When looking at all weekend vs weekday tweet scores as comparing two large group, the mean tweet score on weekends is not significantly different than the mean tweet score on non-weekends, *without* stratifying on tweet volume.

ggplot(mb_sentiment_tweet, aes(x=weekend_binary, y=score_tweet, group=weekend_binary)) +
  geom_boxplot(aes(fill=weekend_binary)) +
  xlab("Non-weekend vs weekend") + 
  ylab("Tweet sentiment score") + 
  ggtitle("Variation in tweet sentiment between weekends and non-weekends") + 
  geom_jitter(colour="gray40",
              position=position_jitter(width=0.2), alpha=0.3) 

#Hyp. #3b: sentiment weekend = sentiment weekday (each weekend day is weighted equally, without regard for tweet volume on that day)
the_weekend_date = mb_sentiment_date %>% filter(weekend_binary == 1)
not_the_weekend_date = mb_sentiment_date %>% filter(weekend_binary == 0)
var.test(the_weekend_date$score_date,not_the_weekend_date$score_date)#variances are equal if p-value > 0.05
t.test(the_weekend_date$score_date,not_the_weekend_date$score_date)#,var.equal = TRUE)
#Conclusion: The mean of daily tweet scores on weekends is significantly less than the mean tweet score on non-weekends, *without* stratifying on tweet volume.

ggplot(mb_sentiment_date, aes(x=weekend_binary, y=score_date, group=weekend_binary)) +
  geom_boxplot(aes(fill=weekend_binary)) +
  xlab("Non-weekend vs. weekend") + 
  ylab("Tweet sentiment score") + 
  ggtitle("Variation in tweet sentiment between weekends and non-weekends") + 
  geom_jitter(colour="gray40",
              position=position_jitter(width=0.2), alpha=0.3) 
```

For the following test, we pursued the association between average sentiment score for tweet-days and weekend status. We performed the same test as above but added tweet volume as a covariate.
```{r}
#Hyp. #4: sentiment weekend = sentiment weekday when stratifying on freq.
#[multiple linear regression]
fit <- lm(score_date ~ weekend_binary + freq, data=mb_sentiment_date)
summary(fit) # show results
#DO NOT HAVE ENOUGH EVIDENCE TO REJECT THE NULL, conclusion:  The weekend is not a statistically signiificant predictor of average daily sentiment score when we stratify on tweet volume.
```

One of our earliest hypotheses was that tweeting would increase on weekends. We conducted a t-test to test this hypothesis.

```{r}
#Hyp. #5: tweet volume weekend = tweet volume weekday

var.test(the_weekend_date$freq,not_the_weekend_date$freq)
t.test(the_weekend_date$freq,not_the_weekend_date$freq)

#REJECT THE NULL (STRONGLY), conclude that tweet volume on weekends is statistically differrent thantweet volume on weekdays. 

ggplot(mb_sentiment_date, aes(x=weekend_binary, y=freq, group=weekend_binary)) +
  geom_boxplot(aes(fill=weekend_binary)) +
  xlab("Non-weekend vs. weekend") + 
  ylab("Tweet volume") + 
  ggtitle("Variation in tweet volume between weekends and non-weekends") + 
  geom_jitter(colour="gray40",
              position=position_jitter(width=0.2), alpha=0.3) 

```

We also conducted a t-test to test the hypothesis that tweet sentiment would be significantly different on holidays (federal holidays, Valentine's Day, and Halloween). This test was inspired by an early observation that there appeared to be many more tweets on Valentine's Day than on an average mid-February day. (More on that later.)

```{r}
#Hyp. #6: sentiment holiday = sentiment !holiday
#Without stratifying on volume
holiday = mb_sentiment_holidays %>% filter(holiday == 1)
not_holiday = mb_sentiment_holidays %>% filter(holiday == 0)
var.test(holiday$score_date,not_holiday$score_date)#variances are equal if p-value > 0.05
t.test(holiday$score_date,not_holiday$score_date,var.equal = TRUE)
#There is no statistically significant association between holiday and tweet sentiment when we do not stratify on volume.

#Stratifying on volume
#[multiple linear regression]
fit <- lm(score_date ~ holiday + freq, data=mb_sentiment_holidays)
summary(fit) # show results
#When we stratify on tweet volume, we still see that there is no statistically significant association between holiday and tweet sentiment.
#echart
ggplot(mb_sentiment_holidays, aes(x=holiday, y=score_date, group=holiday)) +
  geom_boxplot(aes(fill=holiday)) +
  xlab("Non-holiday vs. holiday") + 
  ylab("Tweet sentiment score") + 
  ggtitle("Variation in tweet sentiment score between holidays and non-holidays") + 
  geom_jitter(colour="gray40",
              position=position_jitter(width=0.2), alpha=0.3) 



```

####Visual Word Mapping
We built word clouds and word webs to convey the frequency and cooccurrance of words.

```{r}
#Postive and negative word data sets.
positives <- bing_megabus %>%
  filter(sentiment == "positive") %>%
  dplyr::select(word)

negatives = bing_megabus %>%
  filter(sentiment == "negative") %>%
  dplyr::select(word)

head(by_word)
word_list <- by_word %>% dplyr::select(word)
head(word_list)
word_list_negatives <- subset(word_list, word %in% negatives$word)
head(word_list_negatives)
word_list_positives <- subset(word_list, word %in% positives$word)
head(word_list_positives)

# Negative Word Cloud
word_list_negatives <- Corpus(VectorSource(word_list_negatives))
inspect(word_list_negatives)

toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
word_list_negatives <- tm_map(word_list_negatives, toSpace, "/")
word_list_negatives <- tm_map(word_list_negatives, toSpace, "@")
word_list_negatives <- tm_map(word_list_negatives, toSpace, "\\|")

#Build a term-document matrix
dtm <- TermDocumentMatrix(word_list_negatives)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)

# Word Cloud (Negative)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=150, random.order=FALSE, scale=c(4,.2), rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

##### Positive Word Cloud #####
word_list_positives <- Corpus(VectorSource(word_list_positives))
#inspect(word_list_positives)

toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
word_list_positives <- tm_map(word_list_positives, toSpace, "/")
word_list_positives <- tm_map(word_list_positives, toSpace, "@")
word_list_positives <- tm_map(word_list_positives, toSpace, "\\|")
word_list_positives <- tm_map(word_list_positives, removeWords, c("megabus", "the", "and", "https", "you", "t.co", "for", "this", "bus", "that")) 

#Build a term-document matrix
dtm <- TermDocumentMatrix(word_list_positives)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)

# Word Cloud (Positive)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=150, random.order=FALSE, rot.per=0.35, scale=c(4,.2), 
          colors=brewer.pal(8, "Dark2"))
```

Those outlying days, where everyone was tweeting a lot? Now we're ready to see what some of that tweeting was all about.

[*April 13, 2015: 19 injured in Megabus crash on I-65 in Indiana*](http://www.usatoday.com/story/news/nation/2015/04/13/megabus-crash-indiana/25707085/)

```{r}

by_word <- tweets_df_all %>%
  dplyr::select(text, id, date, date2, time, weekend, weekend_binary, month) %>%
  unnest_tokens(word, text) 

word_list <- by_word %>% dplyr::select(word, date2)

word_list <- subset(word_list, word %in% bing_megabus$word)
word_list_date <- word_list %>%
  filter(date2=="04-13-15")
word_list_date <- word_list_date %>% dplyr::select(word)

word_list_date <- Corpus(VectorSource(word_list_date))
#inspect(word_list_date)

toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
word_list_date <- tm_map(word_list_date, toSpace, "/")
word_list_date <- tm_map(word_list_date, toSpace, "@")
word_list_date <- tm_map(word_list_date, toSpace, "\\|")
word_list_date <- tm_map(word_list_date, removeWords, c("megabus", "the", "and", "https", "you", "t.co", "for", "this", "bus", "that")) 

#Build a term-document matrix
dtm <- TermDocumentMatrix(word_list_date)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)


# Word Cloud (Date)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=150, random.order=FALSE, scale=c(4,.2), rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

[*May 13, 2015: â€˜Heroâ€™ passenger subdues gunman who may have tried to take over Megabus*](https://www.washingtonpost.com/news/morning-mix/wp/2015/05/13/hero-passenger-subdues-gunman-who-may-have-tried-to-take-over-megabus/)

```{r}
by_word <- tweets_df_all %>%
  dplyr::select(text, id, date, date2, time, weekend, weekend_binary, month) %>%
  unnest_tokens(word, text) 

word_list <- by_word %>% dplyr::select(word, date2)

word_list <- subset(word_list, word %in% bing_megabus$word)
word_list_date <- word_list %>%
  filter(date2=="05-13-15")
word_list_date <- word_list_date %>% dplyr::select(word)

word_list_date <- Corpus(VectorSource(word_list_date))
#inspect(word_list_date)

toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
word_list_date <- tm_map(word_list_date, toSpace, "/")
word_list_date <- tm_map(word_list_date, toSpace, "@")
word_list_date <- tm_map(word_list_date, toSpace, "\\|")
word_list_date <- tm_map(word_list_date, removeWords, c("megabus", "the", "and", "https", "you", "t.co", "for", "this", "bus", "that")) 

#Build a term-document matrix
dtm <- TermDocumentMatrix(word_list_date)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)

# Word Cloud (Date)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=150, random.order=FALSE, scale=c(4,.2), rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

[*February 21, 2016: The Day My Megabus Caught Fire*](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=newssearch&cd=1&ved=0ahUKEwjw6rqk18DMAhWLbT4KHacICLYQqQIIHCgAMAA&url=http%3A%2F%2Fwww.nytimes.com%2F2016%2F02%2F22%2Ftravel%2Fthe-day-my-megabus-caught-fire.html&usg=AFQjCNGn4yumdmj3dXBvebbspQf6saQwBw&sig2=x1ans1gQL3jwUhE4p5FEEg)

```{r}
by_word <- tweets_df_all %>%
  dplyr::select(text, id, date, date2, time, weekend, weekend_binary, month) %>%
  unnest_tokens(word, text) 

word_list <- by_word %>% dplyr::select(word, date2)

word_list <- subset(word_list, word %in% bing_megabus$word)
word_list_date <- word_list %>%
  filter(date2=="02-21-16")
word_list_date <- word_list_date %>% dplyr::select(word)

word_list_date <- Corpus(VectorSource(word_list_date))
#inspect(word_list_date)

toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
word_list_date <- tm_map(word_list_date, toSpace, "/")
word_list_date <- tm_map(word_list_date, toSpace, "@")
word_list_date <- tm_map(word_list_date, toSpace, "\\|")
word_list_date <- tm_map(word_list_date, removeWords, c("megabus", "the", "and", "https", "you", "t.co", "for", "this", "bus", "that")) 

#Build a term-document matrix
dtm <- TermDocumentMatrix(word_list_date)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)


# Word Cloud (Date)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=150, random.order=FALSE, scale=c(4,.2), rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

#####Valentine's Day, 2015-2016
```{r}
by_word <- tweets_df_all %>%
  dplyr::select(text, id, date, date2, time, weekend, weekend_binary, month) %>%
  unnest_tokens(word, text) 

word_list <- by_word %>% dplyr::select(word, date2)

word_list <- subset(word_list, word %in% bing_megabus$word)
word_list_date <- word_list %>%
  filter(date2=="02-14-16"|date2=="02-14-15")
word_list_date <- word_list_date %>% dplyr::select(word)

word_list_date <- Corpus(VectorSource(word_list_date))
#inspect(word_list_date)

toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
word_list_date <- tm_map(word_list_date, toSpace, "/")
word_list_date <- tm_map(word_list_date, toSpace, "@")
word_list_date <- tm_map(word_list_date, toSpace, "\\|")
word_list_date <- tm_map(word_list_date, removeWords, c("megabus", "the", "and", "https", "you", "t.co", "for", "this", "bus", "that")) 

#Build a term-document matrix
dtm <- TermDocumentMatrix(word_list_date)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)


# Word Cloud (Date)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=150, random.order=FALSE, scale=c(4,.2), rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

This is a map of words that occur in pairs in the entire data set.
# Word Cooccurence
```{r}

word_cooccurences <- by_word %>% dplyr::select(word, id, date2)
word_cooccurences <- subset(word_cooccurences, word %in% bing_megabus$word)

word_cooccurences <- word_cooccurences %>%
  pair_count(id, word, sort = TRUE) %>%
  dplyr::filter(n>25)

set.seed(2016)
word_cooccurences %>%
  filter(n >= 20) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour="gray") +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1.8, size=5) +
  theme_void()
```



### Final Analysis 

We observed a clear inverse correlation between tweet volume and tweet sentiment. That is, on days when there were more tweets, tweet sentiment tended to be lower. This association was very strong, with a highly significant p-value in all instances.

We did not clearly identify a causal pathway for this association, but rather developed tow theories which provide the basis for future research. The first theory is that on days when people have bad experiences related to Megabus, they will tend to tweet more, because often people tweet to complain. The second theory is that on days when there are more Megabuses on the road, there are naturally more tweets due to the larger group of passengers, and that when there is more Megabus activity there will tend to be a higher risk of breakdowns, delays, traffic jams, and other negative experiences.

It is entirely plausible that both of these effects are causing the association, or perhaps just one, or perhaps the association is by chance.

To test these theories, we would need to obtain data on Megabus activity, perhaps by scraping a travel site. Then we could determine if tweet activity is primarily an indicator Megabus activity, or a downstream consequence of negative Megabus experiences, or both.
