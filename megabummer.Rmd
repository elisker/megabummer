---
title: "Megabummer"
output: html_document
---

### Data Science BIO 260 Final Project, Spring 2016
##### Allison Blajda, Leo Brown, and Emily Lisker

### Overview and Motivation

Social media users leave digital traces that can be leveraged to guide consumer purchasing decisions. Twitter users, in particular, provide temporal data that can be analyzed to develop a more precise understanding of usersâ€™ perspectives or sentiments about a service, company, or product. This information, in turn, can be shared with the public to allow customers to make decisions based on crowd-sourced knowledge. It may also even help the service provider understand how they can improve their services.

Imagine it is Friday in July and you are finishing up a long week at the office. The clock reads 4:00pm and you quickly race to pack up your work station. You need to get downtown to catch the 5pm MegaBus from NYC to Washington, DC. You are ready to start the weekend and have plans at 10pm in downtown DC. You naviate the NYC subway and break a sweat walking quickly down 34th Street, hoping that you are early enough to score an aisle seat. As you approach the bus stop you come into view of your worst nightmare: a line with hundreds of angry Megabus passengers waiting with no bus in sight. As you walk towards the end of the line another passenger mentions that the 3:30pm bus hasn't arrived yet. A Megabus employee tells you he has no idea when the 5pm bus will depart ("Who knows..."). It is 90 degrees, you are sweating profusely, and you will surely miss your evening plans. You feel helpless. A wronged consumer with no recourse. Then you remember a social media site called Twitter...

Based on our personal experience and the experience of others, we aim to analyze recent tweets to understand the degree of positive and negative sentiment among users of the low-cost bus company, Megabus, over time. While Megabus is an inexpensive and convenient transportation option with many buses departing major cities each day, we hypothesized that some days or times of year may be more reliable, i.e., buses have fewer breakdowns and delays, than others. Using social media, data visualization and statistics we provide an informative and entertaining sentiment analysis of Megabus experiences from January 1, 2016 to April 1, 2016. Our work offers future bus riders information on the type of experience they can expect when riding the Megabus.


### Related Work: 

Sentiment analysis performed by our very own TA, David Robinson provided initial guiding inspiration for this this work (https://github.com/juliasilge/tidytext).

We were also inspired by sentiment analysis of Twitter data related to current events. Some examples of other analyses that we explored includes: http://www.wired.com/2015/02/best-worst-public-transit-systems-according-twitter/#slide-2 and https://medium.com/mit-media-lab/introducing-tonar-3bf161cba369#.gixrkibqj

### Initial Questions: 

As a team, we were originally interested in building a predictive model for Megabus experience based on time of day, location and bus route. Based on the data that we were able to collect from Twitter, these questions evolved to focus on the relationship between Megabus passenger sentiment, the volume of Megabus tweets, day of the week, whether the tweet was posted on a weekday (Monday-Thursday) or a weekend (Friday-Sunday), and month of the year.  

The questions we have answered as part of this analysis include the following:
1. How can we most effectively scrape Twitter for data related to Megabus sentiment?
2. What is overall Megabus sentiment and volume of Megabus tweets during the period from January 1, 2015 to April 1, 2016?
3. How does Megabus sentiment and volume of tweets differ based on day of the week and month of the year?
4. What is the realtionship between volume of Megabus tweets on a given day and the overall sentiment we expect to see?

### Data: 

#### Data Scraping and Tidying
We aimed to obtain historical tweet data in order to observe trends in tweet volume and sentiment over time and identify associations with factors such as time of year, month, and day. Since there are many factors influencing tweet volume and sentiment, we wished to obtain the largest possible data set.

Initially, we set up an app (actually, three apps, one for each of us) on the Twitter API that obtained tweets including the word "megabus." This was a great learning experience; we remember fondly this effort as our first true collaboration on git; our first exposure to the Twitter API; when we first set up an .Rprofile to store key sensitive variables; and the first tweet data we obtained. However, it became clear that the Twitter API would only provide us with tweets from the last week. This was insufficient for most of the trends we wished to consider.

A google search led us to [Jefferson Henrique's python package GetOldTweets-python](https://github.com/Jefferson-Henrique/GetOldTweets-python). This package enabled us to obtain tweets from any date (at least, we did not encounter a date that was inaccessible). The original package can be run to print tweets in the terminal. We modified the script so that it would print tweets to a csv file. As a result we obtained over 100,000 tweets from January 1, 2015 through April 1, 2016 containing the word "megabus."

We conducted much of our exploratory analysis using only tweets from the first three months of 2015. Though some questions of interest pertained to the entire 15-month period, we wanted to make sure the data frame was not too large during our exploratory phase. There was initially some evidence that the data frame was too large because some commands were slow. However, it was later identified that the only time-consuming command was an inefficiently written for loop that created several new columns. This was improved to a series of mutate() functions, and since then, wrangling the data frames takes only a few seconds from start to finish.

In order to look at variations in sentiment over days and time, we parsed the combined date and time variable into two new variables, containing just date and time information. We also used the months() function to create a month column and the weekdays() function to create a column that identified the tweet day and another to identify the tweet's status binary weekend status, with Friday counting as part of the weekend.

We cleaned the data using dplyr and the tidytext package, in order to help with the text mining tasks necessary for sentiment analysis, available here: https://github.com/juliasilge/tidytext. 



####Loading all necessary libraries
```{r}
library(dplyr)
library(ggplot2)
library(ggthemes)
library(readr)
library(devtools)
library(tidytext)
library(tidyr)
library(readr)
library(data.table)
library(Hmisc)

library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)

library(devtools)

library(igraph)
library(ggraph)
library(gapminder)
```
####Loading and wrangling the full data set
```{r}
#Increase digits so that the many-digit tweet id does not cause a problem.
options(digits = 22) 
tweets_df_all <- read_csv("Jan_2015-April_2016.csv")
#Change the column names.
names(tweets_df_all) <- c("id","username","text","date","geo","retweets","favorites","mentions","hashtags")

#get a subset of random lines from the full set to work with:
#tweets_df_all <- tweets_df_all[sample(1:nrow(tweets_df_all), 10000, replace=FALSE),]

tweets_df_all <- tweets_df_all %>%
  mutate(time=format(date, format="%H:%M:%S")) %>%
  mutate(date2 = format(date, format="%m-%d-%y")) %>%
  mutate(month = months(as.Date(date2,'%m-%d-%y'))) %>%
  mutate(weekend = weekdays(as.Date(date2,'%m-%d-%y'))) %>%
  mutate(weekend_binary = ifelse(weekend == "Saturday"|weekend=="Sunday"|weekend=="Friday", 1, 0)) %>% 
  filter(date2 != "12-31-14") %>%
  filter(date < "2016-04-01")
# filter out duplicates
tweets_df_all <- tweets_df_all %>%
  distinct(id)
#nrow(tweets_df_all)
tweets_df_all <- tweets_df_all %>%
  distinct(text)
```
####Exploring the data set, part 1 (prior to sentiment analysis)
```{r}
# explore number of tweets per user including megabus handles
prolific_tweeters_all <- tweets_df_all %>% 
  group_by(username) %>%
  summarise(tweets = n()) %>%
  arrange(desc(tweets)) 

# filter out tweets from megabus operators
tweets_df_all = tweets_df_all[!grepl("megabus|megabusuk|MegabusHelp|megabusit|megabusde|megabusGold", tweets_df_all$username),]
prolific_tweeters_filtered <- tweets_df_all %>% 
  group_by(username) %>%
  summarise(tweets = n()) %>%
  arrange(desc(tweets))
```

### Exploratory Analysis

TO BE COMPLETED: What visualizations did you use to look at your data in different ways? What are the different statistical methods you considered? Justify the decisions you made, and show any major changes to your ideas. How did you reach these conclusions?

```{r}
# Histogram of number of tweets
ggplot(filter(prolific_tweeters_filtered, tweets>0), aes(tweets)) + 
  geom_histogram(binwidth = 1) + xlab("Number of megabus tweets per user") + ylab("Number of users") + theme_hc()

# Histogram of number of tweets, for users with more than 8 tweets so that we can get a closer look at the more prolific tweeters
ggplot(filter(prolific_tweeters_filtered, tweets>8), aes(tweets)) + 
  geom_histogram(binwidth = 1) + xlab("Number of megabus tweets per user") + ylab("Number of users") + theme_hc()

#Tweets per day
ggplot(data=tweets_df_all, aes(x=as.Date(date2,'%m-%d-%y'))) + 
  geom_histogram(aes(fill=..count..), binwidth=1) + 
  scale_x_date("Date") + 
  scale_y_continuous("Frequency")

#The outlying days with high tweet volume are:
tweets_df_all %>% group_by(date2) %>% count(date2, sort = TRUE) %>% filter(n>500)
#The outlying days with low tweet volume are:
tweets_df_all %>% group_by(date2) %>% count(date2, sort = TRUE) %>% filter(n<100)
```

On each of the outlying days for high tweet volume, there were significant news stories about Megabus.

[*April 13, 2015: â€˜Heroâ€™ passenger subdues gunman who may have tried to take over Megabus*](https://www.washingtonpost.com/news/morning-mix/wp/2015/05/13/hero-passenger-subdues-gunman-who-may-have-tried-to-take-over-megabus/)

[*May 13, 2015: 19 injured in Megabus crash on I-65 in Indiana*](http://www.usatoday.com/story/news/nation/2015/04/13/megabus-crash-indiana/25707085/)

[*February 21, 2016: The Day My Megabus Caught Fire*](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=newssearch&cd=1&ved=0ahUKEwjw6rqk18DMAhWLbT4KHacICLYQqQIIHCgAMAA&url=http%3A%2F%2Fwww.nytimes.com%2F2016%2F02%2F22%2Ftravel%2Fthe-day-my-megabus-caught-fire.html&usg=AFQjCNGn4yumdmj3dXBvebbspQf6saQwBw&sig2=x1ans1gQL3jwUhE4p5FEEg)



#### Sentiment Analysis
Like most sentiment analyses, we relied on a lexicon of positive and negative words. We started with the Bing lexicon in the tidytext package in the sentiment dataset (described here in more detail https://www.cs.uic.edu/~liub/). This lexicon has been compiled over the past 12 years by researchers at the University of Illinois at Chicago. 

After initial review of the Bing lexicon, we realized that we need to supplement the list of positive and negative words in order to capture the extent of Megabus transportation-specific sentiments. We added a total of 57 sentiment related to Megabus and transportation experience. These additional negative and positive words were identified  by manually reviewing a random sample from 2900 tweets containing the keyword, "megabus" queried on 4/20/2016, following methods similar to those described here: http://www.wired.com/2015/02/best-worst-public-transit-systems-according-twitter/. This review comparing the Bing lexicon with Megabus tweets also revealed that a few sentiments already in the lexicon needed to be recorded or removed. These included: changing "uneventful" from negative to positive, reversing "cheap" from negative to positive given Megabus's platform as a cheap transportation provider, and the removal of "like" which could be positive or negative and is probaby slang.

```{r}
by_word <- tweets_df_all %>%
  dplyr::select(text, id, date, date2, time, weekend, weekend_binary, month) %>%
  unnest_tokens(word, text) 

# look at most commonly tweeted words
by_word_count <- by_word %>%
  count(word, sort = TRUE) 
head(by_word_count)

megabus_lexicon <- read_csv("megabus_lexicon.csv")


# create new dataframe of bing and megabummer sentiments
bing_megabus <- megabus_lexicon %>%
  filter(lexicon %in% c("bing","megabummer")) %>%
  dplyr::select(-score)
head(bing_megabus %>% filter(lexicon=="megabummer"))

# join tweets with sentiment and add score column
mb_sentiment <- by_word %>%
  inner_join(bing_megabus) %>%
  mutate(score = ifelse(sentiment == "positive", 1, -1))
head(mb_sentiment %>% dplyr::select(id,word,sentiment,score))

# calculate score for each tweet
dt <- data.table(mb_sentiment)

#build data set for sentiment analysis containing data on each tweet including sentiment score
mb_sentiment_tweet <- unique(dt[,list(score_tweet = sum(score), freq = .N, date, weekend_binary, date2, weekend, month), by = c("id")] )
tweets_df_all_joiner <- tweets_df_all %>% dplyr::select(id,text)
mb_sentiment_tweet <- left_join(mb_sentiment_tweet,data.table(tweets_df_all_joiner),by="id")
head(mb_sentiment_tweet)

#Creating data table of calendar dates, including weekend status, day of week (column name weekend), month, and tweet frequency and sentiment
mb_sentiment_date <- unique(mb_sentiment_tweet[,list(score_date = round(mean(score_tweet),2), freq = .N, weekend_binary, weekend, month), by = c("date2")] )
mb_sentiment_date <- mb_sentiment_date %>% filter(freq<500)
head(mb_sentiment_date)

#Creating data table of calendar dates and tweet frequency and sentiment with holiday status (including federal holidays, Valentine's Day, and Halloween)
mb_sentiment_holidays <- mb_sentiment_date %>% 
  mutate(holiday = ifelse(date2 == "01-01-15" |
                            date2 == "01-19-15" |
                            date2 == "02-14-15" |
                            date2 == "02-16-15" |
                            date2 == "05-25-15" |
                            date2 == "09-07-15" |
                            date2 == "10-12-15" |
                            date2 == "10-31-15" |
                            date2 == "11-11-15" |
                            date2 == "11-26-15" |
                            date2 == "12-25-15" |
                            date2 == "01-01-16" |
                            date2 == "01-18-16" |
                            date2 == "02-14-16" |
                            date2 == "02-15-16",1,
                          0
                            ))
head(mb_sentiment_holidays)
```
####Exploring the data set, part 2 (sentiment analysis)
The describe() function in Hmisc helped us see how tweet sentiment scores are distributed. In particular, we noticed that the distribution of *by tweet* sentiment scores was similar to the distribution of *by date* sentiment scores (the average sentiment score for that day). We therefore focused our analysis on the *by date* sentiment scores because it was a convenient bin for looking at trends over time and for moderating the considerable variance. The *by date* sentiment score gives a sense for the overall level of sentiment on any given day, which is what we ultimately decided to consider as our main outcome.
```{r}
describe(mb_sentiment_tweet)
describe(mb_sentiment_date)
describe(mb_sentiment_holidays)
```
Looking at a line graph of the sentiment score over time is not particularly useful--it is just a blur with much more variability than any clear trend.
```{r}
ggplot(data=mb_sentiment_tweet, aes(x=date, y=score_tweet)) + 
  geom_line()
```
However, with smoothing, ebbs and flows over time become clear. Though there is much variance day to day, smoothing reveals broad seasonal trends. Smoothing is appropriate for tweet volume and sentiment because the outcome is continuous.
```{r}
options(digits = 3) 
ggplot(data=mb_sentiment_tweet, aes(x=date, y=score_tweet)) + 
  geom_smooth()
options(digits = 22) 
```
This chart shows the distribution of daily sentiment scores. The tweets of most days aired on the negative side, with few days boasting an average tweet score above 0.
```{r}
options(digits = 3)
ggplot(data=mb_sentiment_date, aes(score_date)) + 
  geom_histogram(binwidth = 0.1)
options(digits = 22)
```

####Visual Word Mapping
We built word clouds and word webs to convey the frequency and cooccurrance of words.
```{r}
#Postive and negative word data sets.
positives <- bing_megabus %>%
  filter(sentiment == "positive") %>%
  dplyr::select(word)

negatives = bing_megabus %>%
  filter(sentiment == "negative") %>%
  dplyr::select(word)

head(by_word)
word_list <- by_word %>% dplyr::select(word)
head(word_list)
word_list_negatives <- subset(word_list, word %in% negatives$word)
head(word_list_negatives)
word_list_positives <- subset(word_list, word %in% positives$word)
head(word_list_positives)

# Negative Word Cloud
word_list_negatives <- Corpus(VectorSource(word_list_negatives))
inspect(word_list_negatives)

toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
word_list_negatives <- tm_map(word_list_negatives, toSpace, "/")
word_list_negatives <- tm_map(word_list_negatives, toSpace, "@")
word_list_negatives <- tm_map(word_list_negatives, toSpace, "\\|")

#Build a term-document matrix
dtm <- TermDocumentMatrix(word_list_negatives)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)

# Word Cloud (Negative)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

##### Positive Word Cloud #####
word_list_positives <- Corpus(VectorSource(word_list_positives))
#inspect(word_list_positives)

toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
word_list_positives <- tm_map(word_list_positives, toSpace, "/")
word_list_positives <- tm_map(word_list_positives, toSpace, "@")
word_list_positives <- tm_map(word_list_positives, toSpace, "\\|")
word_list_positives <- tm_map(word_list_positives, removeWords, c("megabus", "the", "and", "https", "you", "t.co", "for", "this", "bus", "that")) 

#Build a term-document matrix
dtm <- TermDocumentMatrix(word_list_positives)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)

# Word Cloud (Positive)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

# Word Cooccurence

word_cooccurences <- by_word %>% dplyr::select(word, id)
word_cooccurences <- subset(word_cooccurences, word %in% bing_megabus$word)

word_cooccurences <- word_cooccurences %>%
  pair_count(id, word, sort = TRUE) %>%
  dplyr::filter(n>25)

set.seed(2016)
word_cooccurences %>%
  filter(n >= 10) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour="gray") +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1.8, size=5) +
  theme_void()
```


### Final Analysis 

TO BE COMPLETED: What did you learn about the data? How did you answer the questions? How can you justify your answers?
